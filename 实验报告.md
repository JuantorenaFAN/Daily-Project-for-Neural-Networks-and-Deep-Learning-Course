# CIFAR-10图像分类三层神经网络实验报告

## 实验概述

本实验基于NumPy手工实现了一个三层神经网络分类器，用于CIFAR-10数据集的图像分类任务。整个实现严格遵循要求，完全避免使用PyTorch、TensorFlow等深度学习框架的自动微分功能，所有的前向传播、反向传播算法均通过数学公式手工推导并编程实现。实验的核心目标是验证手工实现的神经网络在经典图像分类数据集上的有效性，并通过可视化分析网络的学习过程和参数特征。

## 数据集与预处理

CIFAR-10数据集包含60000张32×32像素的彩色图像，涵盖飞机、汽车、鸟类、猫、鹿、狗、青蛙、马、船只和卡车十个类别。数据集被划分为50000张训练图像和10000张测试图像，每个类别均匀分布1000张测试图像。在本实验中，我们进一步从训练集中分离出10%的数据作为验证集，用于模型选择和早停策略，最终形成45000张训练图像、5000张验证图像和10000张测试图像的数据分布。

![CIFAR-10数据集样本](results/data_samples.png)

预处理阶段包含三个关键步骤。首先将原始像素值从0-255的整数范围归一化至0-1的浮点数范围，这一步骤对于神经网络的稳定训练至关重要，能够避免梯度消失或爆炸问题。其次将三维的图像数据展平为一维向量，将每张32×32×3的图像转换为长度为3072的特征向量，以适应全连接神经网络的输入要求。最后将类别标签转换为独热编码形式，将原本的单一整数标签扩展为长度为10的二进制向量，其中正确类别对应位置为1，其余位置为0。

## 网络架构与参数设置

本实验实现的三层神经网络采用经典的全连接架构，包含一个输入层、一个隐藏层和一个输出层。输入层接收展平后的图像特征，包含3072个神经元，对应于32×32×3的像素信息。隐藏层采用64个神经元的配置，在保证模型表达能力的同时控制计算复杂度，激活函数选择ReLU函数，其简单高效的特性有利于梯度传播和模型收敛。输出层包含10个神经元，对应CIFAR-10的十个类别，采用Softmax激活函数将网络输出转换为概率分布。

权重初始化采用Xavier初始化策略，根据输入和输出神经元数量自适应调整初始权重的方差，有效避免了训练初期的梯度消失问题。优化器采用SGD（随机梯度下降）算法，初始学习率设置为0.01，并实施指数衰减策略，每1000步将学习率乘以0.95的衰减因子。损失函数结合交叉熵损失和L2正则化，正则化强度设置为0.001，在防止过拟合的同时保持模型的学习能力。训练过程采用批量大小为64的小批量梯度下降，并实施早停策略，当验证集性能在连续5轮内无改善时停止训练。

## 训练过程分析

训练过程的监控通过损失曲线和准确率曲线进行可视化分析。训练历史曲线图包含三个子图，分别展示了训练和验证损失的变化、训练和验证准确率的对比，以及验证准确率的详细变化趋势。这种多角度的可视化为深入理解模型的学习行为提供了重要依据。

![训练过程可视化](results/training_history.png)

从左侧的损失曲线子图可以观察到，训练损失和验证损失在整个训练过程中呈现出明显的下降趋势。在训练初期，两条曲线几乎重叠，表明模型正在学习数据的基本模式而非过拟合特定样本。随着训练的进行，训练损失从初始的较高数值稳步下降，在第10轮时达到1.6498，第20轮时进一步降至1.5491。验证损失的变化轨迹与训练损失保持相似的趋势，分别在第10轮和第20轮达到1.7180和1.6977。两者之间的差距始终保持在合理范围内，验证损失仅略高于训练损失，这一现象表明模型具备良好的泛化能力，未出现严重的过拟合问题。损失曲线的平滑下降趋势证实了SGD优化器配合学习率衰减策略的有效性，网络能够稳定地向损失函数的最优解收敛。

中间的准确率曲线子图展现了训练准确率和验证准确率的同步提升过程。训练准确率从初期的较低水平逐渐上升，在训练过程中呈现出稳定的增长态势，最终稳定在相对较高的水平。验证准确率的变化模式与训练准确率高度一致，在第10轮时达到41.22%，第20轮时提升至42.00%。两条曲线之间的差距相对较小，训练准确率仅略高于验证准确率，这种模式进一步证实了模型的良好泛化性能。准确率曲线的渐进式上升特征符合神经网络在复杂数据集上的典型学习模式，避免了过度激进的参数更新可能引起的性能振荡。

右侧的验证准确率详细视图提供了对模型性能演化过程的精细分析。该子图通过标记最高准确率点，清晰地展示了模型的最佳性能时刻和数值。验证准确率曲线在训练前期表现出相对陡峭的上升趋势，表明模型在初期学习阶段能够快速掌握数据的基本特征。随着训练的深入，准确率提升的幅度逐渐放缓，呈现出收敛特征。这种变化模式反映了深度学习中常见的学习规律，即模型在训练初期能够快速学习到数据的主要模式，而在后期则需要更多的训练时间来捕捉更加细微的特征差异。曲线上的最高点标注为模型选择提供了明确的参考，自动保存机制确保了最优权重的保留。

训练过程中观察到的另一个重要现象是学习率衰减策略的有效性。随着训练步数的增加，学习率的逐步减小使得模型能够在损失函数的局部最优点附近进行更加精细的参数调整，这对于最终性能的提升具有重要意义。从曲线的光滑性可以看出，学习率衰减有效防止了训练后期可能出现的性能震荡，确保了模型能够稳定地收敛到较优的参数配置。早停策略的实施确保了模型在验证集性能达到稳定后及时停止训练，避免了不必要的计算资源消耗和潜在的过拟合风险。

## 测试结果与性能评估

在独立测试集上的最终评估结果显示，模型达到了45.68%的整体分类准确率。这一结果虽然无法与现代深度卷积神经网络相提并论，但对于仅包含一个隐藏层的全连接网络而言，已经展现出相当可观的性能。相比于随机猜测的10%基准准确率，模型的性能提升了近4.6倍，证明了手工实现的神经网络确实学习到了图像特征与类别标签之间的有效映射关系。

![模型预测结果示例](results/prediction_samples.png)

各类别的分类性能存在显著差异，这种差异反映了不同类别在特征空间中的可分离程度。飞机类别表现最为优异，准确率达到68.30%，这可能与飞机具有相对独特的形状特征和颜色分布有关。汽车和卡车分别达到57.20%和55.80%的准确率，这两个类别的良好性能可能得益于它们相对规整的几何形状和一致的纹理特征。船只类别的准确率为51.20%，略低于车辆类别但仍然保持在较好水平。

相对而言，动物类别的分类性能普遍较低。狗类别的准确率仅为28.70%，是所有类别中最低的，鹿类别的准确率为29.40%，猫类别为31.90%。这种现象可以从多个角度进行解释。首先，动物类别之间在视觉特征上存在更多的相似性，尤其是四足动物在体型轮廓上的共同特征容易导致混淆。其次，动物图像中的个体差异更大，包括毛色、姿态、拍摄角度等变化，增加了分类的难度。再次，全连接网络缺乏卷积神经网络的平移不变性和局部特征提取能力，对于需要识别精细纹理和局部特征的动物分类任务存在固有局限性。

青蛙和马的准确率分别为46.00%和44.80%，处于中等水平，这表明这些类别虽然具有一定的识别难度，但相比其他动物类别仍然保持了相对较好的可区分性。鸟类的准确率为43.50%，考虑到鸟类图像中背景的复杂性和鸟类姿态的多样性，这一结果也在合理范围内。

## 网络参数可视化分析

通过对训练完成后的网络权重参数进行可视化分析，可以深入理解模型的学习机制和特征表示能力。第一层权重矩阵W1的形状为3072×64，连接输入层的每个像素特征与隐藏层的每个神经元。通过热图可视化发现，W1权重分布呈现出明显的空间结构模式。某些隐藏层神经元对应的权重向量在重新排列为32×32×3图像形状时，显示出类似边缘检测器或纹理检测器的特征。权重的数值分布相对均匀，大部分权重集中在-0.5到0.5的范围内，这表明Xavier初始化策略的有效性以及训练过程中梯度传播的稳定性。

![网络权重参数可视化](results/weights_visualization.png)

第一层偏置向量b1包含64个参数，对应每个隐藏层神经元的偏置项。可视化结果显示，大部分偏置值接近零，少数神经元具有较大的正偏置或负偏置，这种模式表明网络通过偏置调整来控制不同神经元的激活阈值，从而实现对不同特征模式的选择性响应。

第二层权重矩阵W2的形状为64×10，连接隐藏层与输出层。这一层的权重模式更加抽象，反映了从低级特征向高级语义概念的转换过程。通过分析W2的每一列（对应一个输出类别），可以观察到不同类别对隐藏层特征的依赖程度。某些类别显示出对特定隐藏层神经元的强依赖，权重值的绝对值较大，而另一些类别则表现出更加分散的权重分布，依赖多个隐藏层特征的综合信息。

第二层偏置向量b2包含10个参数，对应每个输出类别。这些偏置值的分布反映了不同类别在训练数据中的相对频率和固有难度。准确率较高的类别（如飞机）通常具有较大的正偏置，而准确率较低的类别（如狗）可能具有较小甚至负的偏置值。

整体而言，权重参数的可视化结果证实了网络确实学习到了有意义的特征表示。第一层权重捕获了图像的基础视觉特征，如边缘、纹理和颜色模式，第二层权重则将这些基础特征组合为更复杂的语义表示。权重分布的相对均匀性和合理的数值范围表明训练过程的稳定性和收敛性，没有出现梯度消失或梯度爆炸等病理现象。

## 模型局限性与改进方向

虽然本实验的手工实现神经网络在CIFAR-10数据集上取得了合理的性能，但仍然存在一些固有局限性。首先，全连接架构无法有效利用图像数据的空间结构信息，每个像素被独立处理，缺乏对局部特征和空间关系的建模能力。其次，单一隐藏层的网络深度限制了模型的表达能力，难以学习复杂的非线性映射关系。再次，相对较小的隐藏层规模（64个神经元）可能不足以充分表示CIFAR-10数据集的复杂特征空间。

未来的改进方向包括增加网络深度，引入更多隐藏层以增强模型的表达能力；扩大隐藏层规模，提供更丰富的特征表示空间；实施更先进的正则化技术，如Dropout或批量归一化，以进一步防止过拟合；采用更复杂的优化算法，如Adam或RMSprop，以提高训练效率和最终性能。尽管存在这些局限性，本实验成功验证了从基础数学原理出发手工实现神经网络的可行性，为深入理解深度学习算法的内在机制提供了宝贵的实践经验。 

## 代码链接和模型权重

### 代码链接：

### 模型权重：